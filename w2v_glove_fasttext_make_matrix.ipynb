{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization._plotly_imports import _imports\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import re\n",
    "from gensim.models.fasttext import FastText\n",
    "import pickle\n",
    "from glove import Corpus, Glove\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext\n",
    "\n",
    "# worker를 1로 해야 seed고정이 된다.\n",
    "random_state = 42\n",
    "\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "\n",
    "data = pd.read_csv('csv경로', encoding='cp949')\n",
    "# print(data[:30])\n",
    "print('데이터 개수 : ',len(data))\n",
    "x = data['cvrs']\n",
    "y = data['label']\n",
    "\n",
    "# classes = list(set(y))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size =0.2, shuffle=True, random_state=random_state, stratify=y)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "tokenized_train= [word_tokenize(text) for text in x_train]\n",
    "tokenized_test= [word_tokenize(text) for text in x_test]\n",
    "print(tokenized_test[:10])\n",
    "\n",
    "\n",
    "\n",
    "vector_size = 200\n",
    "ft_model = FastText(tokenized_train, \n",
    "                    vector_size=vector_size,\n",
    "                    window=8, \n",
    "                    min_count=8, \n",
    "                    sample=0.001, \n",
    "                    sg=1, \n",
    "                    epochs=16,\n",
    "                    workers=1,\n",
    "                    seed = random_state)  # 훈련 재현성을 높이기 위해 가중치를 랜덤하게 초기화하는 데 사용되는 해시 함수\n",
    "\n",
    "filepath = '파일루트'\n",
    "filename = filepath + '파일이름.pkl'\n",
    "pickle.dump(ft_model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def document_vectorizer(corpus, model, num_features):\n",
    "    # 고유 단어 집합\n",
    "    vocabulary = set(model.wv.key_to_index)\n",
    "    # print(len(vocabulary))\n",
    "    # print(vocabulary) # {'교환', '육개', '연휴', ... ,'십일만', '서비스'}\n",
    "\n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        # 모델의 vector_size만큼 0으로 채워진 테이블 생성\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "\n",
    "        nwords = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                # vocab에 속한 단어의 벡터값 모두 더해서 feature_vector에 넣어줌\n",
    "                feature_vector = np.add(feature_vector, model.wv[word]) \n",
    "\n",
    "        if nwords:\n",
    "            # vocab에 속한 단어 개수만큼 나눠서 평균 구함\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# generate averaged word vector features from word2vec model\n",
    "X_TRAIN = document_vectorizer(corpus=tokenized_train, model=ft_model, num_features=vector_size)\n",
    "X_TEST = document_vectorizer(corpus=tokenized_test, model=ft_model, num_features=vector_size)\n",
    "\n",
    "print(f'Word2Vec model:> Train features shape: \\t {X_TRAIN.shape}\\n \\\n",
    "                         Test features shape: \\t {X_TEST.shape}')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# model = SGDClassifier(alpha=0.00001, loss='log', penalty='l2', n_jobs=-1, verbose=0, random_state=random_state, n_iter_no_change=8, validation_fraction=0.2)\n",
    "model = SGDClassifier(alpha=0.0001, loss='modified_huber', penalty='l2', n_jobs=-1, verbose=0,random_state=random_state,  n_iter_no_change=8, validation_fraction=0.2)\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(X_TRAIN, y_train)\n",
    "end = datetime.now()\n",
    "\n",
    "\n",
    "print('시간 : ', end-start)\n",
    "\n",
    "# svm_w2v_cv_scores = cross_val_score(model, X_TRAIN, y_train, cv=20)\n",
    "# svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "# print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "# print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "\n",
    "test_accuracy = model.score(X_TEST, y_test)\n",
    "train_accuracy = model.score(X_TRAIN, y_train)\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "y_pred = model.predict(X_TEST)\n",
    "\n",
    "test_report = classification_report(y_pred, y_test, zero_division=0)\n",
    "print('test_report : \\n',test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "\n",
    "random_state = 64\n",
    "\n",
    "data = pd.read_csv('csv경로', encoding='cp949')\n",
    "# print(data[:30])\n",
    "print('데이터 개수 : ',len(data))\n",
    "x = data['cvrs']\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size =0.2, shuffle=True, random_state=random_state, stratify=y)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "tokenized_train= [word_tokenize(text) for text in x_train]\n",
    "tokenized_test= [word_tokenize(text) for text in x_test]\n",
    "\n",
    "# print(tokenized_test[:10])\n",
    "# print(len(tokenized_test))\n",
    "\n",
    "\n",
    "# corpus 생성\n",
    "corpus = Corpus()\n",
    "corpus.fit(tokenized_train, window=5)\n",
    "\n",
    "# 경사하강법 학습률 0.05, 아웃풋 벡터의 차원 100\n",
    "glove = Glove(no_components=200, learning_rate=0.1)\n",
    "# 쓰레드 개수는 4개, 훈련 횟수는 20번, verbose (설명) True\n",
    "glove.fit(corpus.matrix, epochs=64, no_threads=4, verbose=False)\n",
    "# 유사도 검색을 위한 행렬의 index 정보 입력\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "glove.save('경로.model')\n",
    "\n",
    "glove_model = glove.load('경로.model')\n",
    "\n",
    "def document_vectorizer(corpus, model, num_features):\n",
    "    # 고유 단어 집합\n",
    "    vocabulary = set(glove_model.dictionary.keys())\n",
    "    # print(len(vocabulary)) # 3869\n",
    "    # print(vocabulary) # {'교환', '육개', '연휴', ... ,'십일만', '서비스'}\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        # 모델의 vector_size만큼 0으로 채워진 테이블 생성\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "\n",
    "        nwords = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                # vocab에 속한 단어의 벡터값 모두 더해서 feature_vector에 넣어줌\n",
    "                feature_vector = np.add(feature_vector, model.word_vectors[model.dictionary[word]])\n",
    "\n",
    "        if nwords:\n",
    "            # vocab에 속한 단어 개수만큼 나눠서 평균 구함\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "X_TRAIN = document_vectorizer(corpus=tokenized_train, model=glove_model, num_features=200)\n",
    "X_TEST = document_vectorizer(corpus=tokenized_test, model=glove_model, num_features=200)\n",
    "\n",
    "\n",
    "print(f'Word2Vec model:> Train features shape: \\t {X_TRAIN.shape}\\n \\\n",
    "                         Test features shape: \\t {X_TEST.shape}')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "model = SGDClassifier(alpha=0.00001, loss='log', penalty='l2', n_jobs=-1, verbose=0, random_state=random_state, n_iter_no_change=8, validation_fraction=0.2)\n",
    "# model = SVC(class_weight='balanced',  random_state=random_state)\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(X_TRAIN, y_train)\n",
    "end = datetime.now()\n",
    "\n",
    "print('시간 : ', end-start)\n",
    "\n",
    "# svm_w2v_cv_scores = cross_val_score(model, X_TRAIN, y_train, cv=20)\n",
    "# svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "# print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "# print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "\n",
    "test_accuracy = model.score(X_TEST, y_test)\n",
    "train_accuracy = model.score(X_TRAIN, y_train)\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "y_pred = model.predict(X_TEST)\n",
    "\n",
    "test_report = classification_report(y_pred, y_test, zero_division=0)\n",
    "print('test_report : \\n',test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v\n",
    "\n",
    "random.seed(64)\n",
    "np.random.seed(64)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(64)\n",
    "\n",
    "random_state = 64\n",
    "\n",
    "data = pd.read_csv('csv경로', encoding='cp949')\n",
    "# print(data[:30])\n",
    "print('데이터 개수 : ',len(data))\n",
    "x = data['cvrs']\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size =0.2, shuffle=True, random_state=random_state, stratify=y)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "tokenized_train= [word_tokenize(text) for text in x_train]\n",
    "tokenized_test= [word_tokenize(text) for text in x_test]\n",
    "\n",
    "# w2v_model = KeyedVectors.load_word2vec_format('모델경로')\n",
    "\n",
    "vector_size = 200\n",
    "w2v_model = Word2Vec(sentences=tokenized_train,\n",
    "                vector_size=vector_size,\n",
    "                window=2,\n",
    "                min_count=128, # 값을 작게할 수록 몇번 안나온 단어들도 계산에 포함, 속도가 느려지고, 모델의 크기가 커진다\n",
    "                workers=4, # -1로 하면 빨리 나오지만 성능 떨어짐\n",
    "                sg=1, #skip gram = 1, cbow = 0\n",
    "                seed=64,\n",
    "                negative = 5, # 희귀한 단어가 샘플로 조금 더 잘 뽑힐 수 있도록 한다.\n",
    "                epochs=64)\n",
    "\n",
    "def document_vectorizer(corpus, model, num_features):\n",
    "    # 고유 단어 집합\n",
    "    vocabulary = set(model.wv.key_to_index)\n",
    "    # print(len(vocabulary))\n",
    "    # print(vocabulary) # {'교환', '육개', '연휴', ... ,'십일만', '서비스'}\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        # 모델의 vector_size만큼 0으로 채워진 테이블 생성\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "\n",
    "        nwords = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                # vocab에 속한 단어의 벡터값 모두 더해서 feature_vector에 넣어줌\n",
    "                feature_vector = np.add(feature_vector, model.wv[word]) \n",
    "\n",
    "        if nwords:\n",
    "            # vocab에 속한 단어 개수만큼 나눠서 평균 구함\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# generate averaged word vector features from word2vec model\n",
    "X_TRAIN = document_vectorizer(corpus=tokenized_train, model=w2v_model, num_features=vector_size)\n",
    "X_TEST = document_vectorizer(corpus=tokenized_test, model=w2v_model, num_features=vector_size)\n",
    "\n",
    "print(f'Word2Vec model:> Train features shape: \\t {X_TRAIN.shape}\\n \\\n",
    "                         Test features shape: \\t {X_TEST.shape}')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# model = SGDClassifier(alpha=0.00001, loss='log', penalty='l2', n_jobs=-1, verbose=0, random_state=random_state, n_iter_no_change=8, validation_fraction=0.2)\n",
    "model = SGDClassifier(alpha=0.0001, loss='hinge', penalty='l2', n_jobs=-1, verbose=0, random_state=random_state, n_iter_no_change=8, validation_fraction=0.2)\n",
    "\n",
    "start = datetime.now()\n",
    "model.fit(X_TRAIN, y_train)\n",
    "end = datetime.now()\n",
    "\n",
    "print('시간 : ', end-start)\n",
    "\n",
    "# svm_w2v_cv_scores = cross_val_score(model, X_TRAIN, y_train, cv=20)\n",
    "# svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "# print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "# print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "\n",
    "test_accuracy = model.score(X_TEST, y_test)\n",
    "train_accuracy = model.score(X_TRAIN, y_train)\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "y_pred = model.predict(X_TEST)\n",
    "\n",
    "test_report = classification_report(y_pred, y_test, zero_division=0)\n",
    "print('test_report : \\n',test_report)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
