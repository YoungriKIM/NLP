{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ntdev/youngri/YR_LAB/env/yr3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data_path = '/ntdev/youngri/YR_LAB/pytorch_textclassification/dataset/news.csv'\n",
    "destination_folder = '/ntdev/youngri/YR_LAB/pytorch_textclassification/destination'\n",
    "\n",
    "train_test_ratio = 0.1\n",
    "train_valid_ratio = 0.8\n",
    "\n",
    "first_n_words = 200\n",
    "\n",
    "# 빠른 학습을 위해 데이터를 자를 함수 생성\n",
    "def trim_string(x):\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>4490</td>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>8062</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>8622</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>4021</td>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>4330</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           8476                       You Can Smell Hillary’s Fear   \n",
       "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4            875   The Battle of New York: Why This Primary Matters   \n",
       "...          ...                                                ...   \n",
       "6330        4490  State Department says it can't find emails fro...   \n",
       "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                                   text label  \n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4     It's primary day in New York and front-runners...  REAL  \n",
       "...                                                 ...   ...  \n",
       "6330  The State Department told the Republican Natio...  REAL  \n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
       "\n",
       "[6335 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본 데이터\n",
    "df_raw = pd.read_csv(raw_data_path)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- done --\n"
     ]
    }
   ],
   "source": [
    "# 열 정리\n",
    "df_raw['label'] = (df_raw['label'] == 'FAKE').astype('int') # REAL to 0 and FAKE to 1\n",
    "df_raw['titletext'] = df_raw['title'] + \".\" + df_raw['text']\n",
    "df_raw = df_raw.reindex(columns=['label', 'title', 'text', 'titletext'])\n",
    "\n",
    "# 빈 값 삭제\n",
    "df_raw.drop(df_raw[df_raw.text.str.len() < 5].index, inplace=True)\n",
    "\n",
    "# 트림 함수 적용\n",
    "df_raw['text'] = df_raw['text'].apply(trim_string)\n",
    "df_raw['titletext'] = df_raw['titletext'].apply(trim_string)\n",
    "\n",
    "# 라벨으로 나누기\n",
    "df_real = df_raw[df_raw['label'] == 0]\n",
    "df_fake = df_raw[df_raw['label'] == 1]\n",
    "\n",
    "# t / tr split\n",
    "df_real_full_train, df_real_test = train_test_split(df_real, train_size=train_test_ratio, random_state=1)\n",
    "df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size=train_test_ratio, random_state=1)\n",
    "\n",
    "# t / v split\n",
    "df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state=1)\n",
    "df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state=1)\n",
    "\n",
    "# tr/ v/ t concatenate\n",
    "df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\n",
    "df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\n",
    "df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n",
    "\n",
    "# save df data\n",
    "df_train.to_csv(destination_folder + '/train.csv', index=False)\n",
    "df_valid.to_csv(destination_folder + '/valid.csv', index=False)\n",
    "df_test.to_csv(destination_folder + '/test.csv', index=False)\n",
    "\n",
    "print('-- done --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>titletext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Obamacare's unlikely No. 1 city</td>\n",
       "      <td>Killing Obama administration rules, dismantlin...</td>\n",
       "      <td>Obamacare's unlikely No. 1 city.Killing Obama ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>New York restores order for 2016 front-runners</td>\n",
       "      <td>Hillary Clinton and Donald Trump scored resoun...</td>\n",
       "      <td>New York restores order for 2016 front-runners...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           title  \\\n",
       "0      0                 Obamacare's unlikely No. 1 city   \n",
       "1      0  New York restores order for 2016 front-runners   \n",
       "\n",
       "                                                text  \\\n",
       "0  Killing Obama administration rules, dismantlin...   \n",
       "1  Hillary Clinton and Donald Trump scored resoun...   \n",
       "\n",
       "                                           titletext  \n",
       "0  Obamacare's unlikely No. 1 city.Killing Obama ...  \n",
       "1  New York restores order for 2016 front-runners...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    " \n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device('cuda')\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields 이용해 csv 파일을 사용할 수 있도록 tr/v/t 으로 나누면서 설정\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
    "fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
    "\n",
    "# TabularDataset으로 테이블 형태로 데이터 설정\n",
    "train, valid, test = TabularDataset.splits(path=destination_folder, train='train.csv', validation='valid.csv', test='test.csv',\n",
    "                                            format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "# iterators 생성-batch learning 기능. 비슷한 길이를 가진 텍스트를 한 배치에 할당하여 패딩을 최소화\n",
    "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text), device=device, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.text), device=device, sort=True, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text), device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "# 사전 생성(최소 빈도 3이상 경우만)\n",
    "text_field.build_vocab(train, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(text_field.vocab), 300)   # 300크기로 임베딩\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True) # 양방향으로 설정\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(2*dimension, 1) # full connect를 위해 1개의 차원으로 펼치기\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        text_emb = self.embedding(text)\n",
    "        # pack_padded_sequence = 패딩된 문장을 패딩 기준으로 정렬해주는 역할\n",
    "        # 참고: https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html\n",
    "        packed_input = pack_padded_sequence(text_emb,\n",
    "                                            text_len.cpu(),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text.fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 필요한 함수 생성\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "    if save_path == None: return\n",
    "\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "    if load_path == None: return\n",
    "\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model load from <== {load_path}')\n",
    "\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "    if save_path == None: return\n",
    "\n",
    "    state_dict = {'train_loss_list':train_loss_list,\n",
    "                  'valid_loss_list':valid_loss_list,\n",
    "                  'global_steps_list':global_steps_list}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_metrics(load_path):\n",
    "    if load_path == None: return\n",
    "\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 함수를 만들자\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion = nn.BCELoss(), # binary case 인 경우에 사용하는 Loss로 softmax를 포함하지 않고 cross entropy만 구한다.\n",
    "    train_loader = train_iter,\n",
    "    valid_loader = valid_iter,\n",
    "    num_epochs = 5,\n",
    "    eval_every = len(train_iter) // 2,\n",
    "    file_path = destination_folder,\n",
    "    best_valid_loss = float('Inf')\n",
    "    ):\n",
    "\n",
    "    # running values 초기화\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list, valid_loss_list, global_step_list = [], [], []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            titletext = titletext.to(device)\n",
    "            titletext_len = titletext_len.to(device)\n",
    "            output = model(titletext, titletext_len)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # validation loop\n",
    "                    for (labels, (title, title_len), (text, text_len), (titletext, titletext_len)), _ in valid_loader:\n",
    "                        labels = labels.to(device)\n",
    "                        titletext = titletext.to(device)\n",
    "                        titletext_len = titletext_len.to(device)\n",
    "                        output = model(titletext, titletext_len)\n",
    "\n",
    "                        loss = criterion(output, labels)\n",
    "                        valid_running_loss += loss.item()\n",
    "                \n",
    "                # evaluation\n",
    "                avarage_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(avarage_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_step_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0\n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss {:.4f}, Valid Loss {:.4f}'\\\n",
    "                    .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader), avarage_train_loss, average_valid_loss))\n",
    "\n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                    save_metrics(file_path, '/metrics.pt', train_loss_list, valid_loss_list, global_step_list)\n",
    "\n",
    "    save_metrics(file_path, '/metrics.pt', train_loss_list, valid_loss_list, global_step_list)\n",
    "    print('--Done--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTM().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train(model=model, optimizer=optimizer, num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed2d5ed8945374f5d0a9e128ab6a7824b02d2cd8bc8c56780f1bbf226ece0c30"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('w2venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
